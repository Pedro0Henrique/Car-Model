# -*- coding: utf-8 -*-
"""Car Model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BrD0e0qqQLCtY7pmnf_mA7_S-gikEIp1
"""

# importa as bibliotecas necessárias
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import VGG16
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.metrics import confusion_matrix, classification_report
import numpy as np

# Função para redimensionar uma imagem
def preprocess_image(image):
    # Redimensionar a imagem para o tamanho padrão da VGG16
    image = tf.image.resize(image, (224, 224))
    return image

# Monta o Google Drive para acessar os dados
from google.colab import drive
drive.mount('/content/drive')

# Definir o caminho para as pastas de treinamento, validação e teste no Google Drive
train_dir = '/content/drive/My Drive/Datasets/TrafficSigns/Train'
validation_dir = '/content/drive/My Drive/Datasets/TrafficSigns/Validation'
test_dir = '/content/drive/My Drive/Datasets/TrafficSigns/Test'

# Definir hiperparâmetros
batch_size = 64
epochs = 100
num_classes = 4

# Configurar geradores de dados para treinamento, validação e teste
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest')

validation_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

# Pré-processamento personalizado para o VGG16
preprocess_input = preprocess_input

# geradores de dados e aplica o pré-processamento manualmente
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(224, 224),
    batch_size=batch_size,
    class_mode='categorical')

validation_generator = validation_datagen.flow_from_directory(
    validation_dir,
    target_size=(224, 224),
    batch_size=batch_size,
    class_mode='categorical')

test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(224, 224),
    batch_size=batch_size,
    class_mode='categorical')

# Construindo o modelo (descarta as camadas totalmente conectadas e, em seguida, adiciona uma camada de pooling global )
base_model = VGG16(weights='imagenet', include_top=False)
x = base_model.output
x = GlobalAveragePooling2D()(x)

# Adiciona camadas densas adicionais e camadas de dropout
x = Dense(512, activation='relu')(x)
x = Dropout(0.2)(x)

x = Dense(256, activation='relu', kernel_regularizer=l2(0.01))(x) #camada final utiliza regularização L2
x = Dropout(0.2)(x)

# Camada de saída para classificação
predictions = Dense(num_classes, activation='softmax')(x)

# Construir o modelo completo (combinando a base vgg16 e as camadas densas)
model = Model(inputs=base_model.input, outputs=predictions)

# Compila o modelo com otimizador, função de perda e métricas
model.compile(optimizer=Adam(learning_rate=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])

# Define EarlyStopping e ModelCheckpoint
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
model_checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True)

# Treinar o modelo com dados de treinamento e validação
history = model.fit(
    train_generator,
    steps_per_epoch=len(train_generator),
    epochs=epochs,
    validation_data=validation_generator,
    validation_steps=len(validation_generator),
    callbacks=[early_stopping, model_checkpoint])  # Adiciona callbacks

# Avaliar o modelo no conjunto de teste
test_loss, test_acc = model.evaluate(test_generator)
print("Acurácia no conjunto de teste:", test_acc)

# Carregar os melhores pesos do modelo (de acordo com o modelcheckpoint)
model.load_weights('best_model.h5')

# Gerar previsões no conjunto de teste e converte nas classes com maior probabilidade
predictions = model.predict(test_generator)
y_pred = np.argmax(predictions, axis=1)

# Obter as classes reais do conjunto de teste
class_names = list(test_generator.class_indices.keys())
y_true = test_generator.classes

# Calcular métricas (calcula a matriz usando as classes reais e as previsoes)
confusion = confusion_matrix(y_true, y_pred)
classification_report = classification_report(y_true, y_pred, target_names=class_names)

# Imprimir a matriz de confusão e relatório de classificação
print('Matriz de Confusão:')
print(confusion)
print('Relatório de Classificação:')
print(classification_report)